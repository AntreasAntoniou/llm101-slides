# LLM 101 — Efficiency, Compression & Distillation

Interactive slide deck for the lecture **"LLM 101: Efficiency, Compression & Distillation"** by Antreas Antoniou.

## View

Open `index.html` in any browser, or visit the live hosted version.

## Topics Covered

- The Transformer architecture — self-attention, positional embeddings, skip connections
- Training at scale — FSDP, mixed precision, distributed training
- Model compression — pruning, quantization, knowledge distillation
- Distillation deep-dives — temperature trick, the conundrum, speculative decoding
- Smarter, not bigger — PEFT, LoRA, local deployment with Ollama
- 2026 research — Invariant Algorithmic Cores, Koopman spectral profiling, RMT pruning

## Contact

- antreas@axiotic.ai
- iam@antreas.io
- [@AntreasAntoniou](https://x.com/AntreasAntoniou)
- [axiotic.ai](https://axiotic.ai)
